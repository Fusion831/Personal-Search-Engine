# Personal Search Engine

> **A production-grade, full-stack RAG system with advanced multi-stage retrieval, intelligent query routing, and hierarchical chunking ‚Äî built using the Google Gemini Python SDK.**

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com/)
[![React](https://img.shields.io/badge/React-18+-61DAFB.svg)](https://reactjs.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-336791.svg)](https://www.postgresql.org/)

---

## üéØ The Problem

Traditional RAG (Retrieval-Augmented Generation) systems suffer from critical limitations:

- **Precision-Context Trade-off**: Small chunks improve retrieval accuracy but lose context; large chunks provide context but reduce precision
- **Semantic Gap**: User queries often don't semantically align with how information is written in documents
- **Query Ambiguity**: No distinction between broad exploratory questions ("What is this document about?") and specific factual queries ("What was the accuracy of Model X?")
- **Poor User Experience**: Synchronous processing blocks users during document upload and indexing

## üí° The Solution

This system implements a **multi-stage, production-grade RAG pipeline** that solves these problems through:

1. **Hierarchical Parent-Child Chunking**: Retrieves precise child chunks but returns full parent paragraphs for context
2. **Query Transformation (HyDe)**: Transforms queries into hypothetical answers before embedding to bridge the semantic gap
3. **Intelligent Query Routing**: Automatically routes broad queries to document summaries and specific queries to granular chunks
4. **Asynchronous Processing**: Non-blocking document ingestion with Celery workers

---

## üñºÔ∏è Application Interface

### Main Interface
![Application Interface](./images/Initial_Frontend.png)
*Modern React interface with document management and conversational search*

### Query Response
![Query Response](./images/Question_Answering.png)
*Real-time streaming responses with intelligent context routing*

---

## ‚ú® Key Features

### üîç Advanced Retrieval Architecture

- **Parent-Child Hierarchical Chunking**
  - Small child chunks (500 chars) for precise vector search
  - Full parent paragraphs returned for comprehensive context
  - Eliminates precision-context trade-off
  - Unique parent deduplication to optimize token usage

- **HyDe Query Transformation**
  - Transforms user queries into hypothetical document passages
  - Bridges semantic gap between questions and answers
  - Uses Gemini API to generate contextually rich query representations
  - Significantly improves retrieval accuracy on ambiguous queries

- **Intelligent Query Routing**
  - Compares similarity scores between document summaries and child chunks
  - Routes broad/exploratory queries ‚Üí Document summaries
  - Routes specific/factual queries ‚Üí Parent-child retrieval
  - Dynamic threshold-based decision making (configurable)

### üèóÔ∏è Production-Grade Infrastructure

- **Asynchronous Document Processing**
  - Celery workers with Redis broker
  - Non-blocking uploads with background processing
  - Real-time task status tracking
  - Batch embedding generation for efficiency

- **Scalable Vector Storage**
  - PostgreSQL with pgvector extension
  - Optimized L2 distance queries
  - Separate tables for documents, parent chunks, child chunks, and summaries
  - Foreign key relationships for data integrity

- **Direct AI Integration**
  - Gemini Python SDK for generation and summarization
  - Custom prompt engineering for each pipeline stage
  - Streaming responses for better UX
  - Server-Sent Events (SSE) for real-time updates

### üé® Modern Full-Stack Application

- **React Frontend**: Modern UI with real-time streaming responses
- **FastAPI Backend**: High-performance async Python framework
- **Docker Compose**: One-command deployment
- **Multi-file Upload**: Process multiple PDFs simultaneously

---

## üèõÔ∏è Technical Architecture

### Workflow 1: Asynchronous Data Ingestion Pipeline

This workflow handles document uploads, processing, and indexing without blocking the user.

![Document Ingestion Pipeline](./images/DocumentIngestion.drawio.png)

**Key Implementation Details**:
- Uses `pypdf` for text extraction with regex-based cleaning
- Gemini API generates comprehensive document summaries
- `sentence-transformers/all-MiniLM-L6-v2` creates 384-dim embeddings
- Parent chunks filtered by minimum length (50+ chars) to remove artifacts
- Child chunks created with 100-character overlap for context continuity
- All embeddings stored as pgvector types for native similarity queries
- Celery workers process tasks asynchronously with Redis broker

**Database Schema**:
```
documents      ‚Üí Metadata (id, title)
  ‚Üì
summaries      ‚Üí Summary text + embedding (document_id FK)
  ‚Üì
parents        ‚Üí Full paragraphs (document_id FK)
  ‚Üì
children       ‚Üí Small chunks + embeddings (parent_id FK)
```

---

### Workflow 2: Multi-Stage Retrieval & Generation Pipeline

This workflow handles user queries with a sophisticated 3-stage process.

![Query Processing Pipeline](./images/QuestionAnswering.drawio.png)

**Stage 1: Query Transformation (HyDe)**
```python
# Original user query
"What is diabetes?"

# Gemini API transforms to hypothetical answer
"Diabetes is a chronic condition where the body cannot properly 
regulate blood sugar levels due to insulin resistance or deficiency..."

# Encode transformed text for better semantic matching
queryVector = embed(transformed_text)
```

**Stage 2: Intelligent Routing**
- Performs parallel search on both summaries and child chunks
- Compares L2 distances to determine query type
- Routes broad questions to summaries
- Routes specific questions to parent-child retrieval

**Stage 3: Context-Aware Generation**
- Summary route: Uses full document summary as context
- Parent-child route: Combines child chunks + unique parent paragraphs
- Gemini API generates streaming response
- Frontend renders markdown progressively

**Key Implementation Details**:
- HyDe transformation uses custom prompt engineering with Gemini
- Query routing threshold (0.8) is configurable and empirically tuned
- Parent deduplication uses `set()` to eliminate redundant context
- Streaming uses Server-Sent Events (SSE) for real-time updates
- All prompts designed to prevent hallucination and enforce grounding

---

## ÔøΩÔ∏è Tech Stack

### Backend
- **FastAPI** - Modern async web framework with automatic OpenAPI docs
- **Celery** - Distributed task queue for async processing
- **Redis** - Message broker and result backend for Celery
- **PostgreSQL 16** - Primary relational database
- **pgvector** - Vector similarity search extension for PostgreSQL
- **SQLAlchemy** - ORM with typed mappings
- **Pydantic** - Data validation and settings management

### AI/ML
- **Google Gemini Python SDK** - Direct API integration for generation
- **Sentence-Transformers** - Local embedding generation (`all-MiniLM-L6-v2`)
- **pypdf** - PDF text extraction
- **NumPy** - Vector operations and distance calculations

### Frontend
- **React 18** - Modern UI library with hooks
- **Vite** - Fast build tool and dev server
- **CSS Modules** - Scoped styling
- **Fetch API** - Streaming response handling

### DevOps
- **Docker** - Containerization
- **Docker Compose** - Multi-container orchestration
- **Nginx** - Reverse proxy for production deployment

---

## üöÄ Setup & Installation

### Prerequisites
- Docker & Docker Compose (recommended)
- OR: Python 3.11+, Node.js 20+, PostgreSQL 16, Redis

### Environment Variables

Create a `.env` file in the backend directory:

```bash
# Google Gemini API
GOOGLE_API_KEY=your_gemini_api_key_here

# Database
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_secure_password
POSTGRES_DB=personal_search_engine

# Celery
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0
```

### Quick Start with Docker (Recommended)

```bash
# Clone the repository
git clone https://github.com/Fusion831/Personal-Search-Engine.git
cd Personal-Search-Engine

# Create .env file in backend directory with your credentials
# Add your GOOGLE_API_KEY

# Build and start all services
docker-compose up --build

# Access the application
# Frontend: http://localhost:5173
# Backend API: http://localhost:8000
# API Docs: http://localhost:8000/docs
```

The application will start:
- **Frontend** on port 5173
- **Backend API** on port 8000
- **PostgreSQL** on port 5432
- **Redis** on port 6379
- **Celery Worker** (background processing)

### Manual Setup (Development)

#### Backend

```bash
cd backend

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up environment variables in .env file

# Initialize database
python -c "from database import engine; from models import Base; Base.metadata.create_all(bind=engine)"

# Start FastAPI server
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# In a separate terminal, start Celery worker
celery -A worker.celery_app worker --loglevel=info
```

#### Frontend

```bash
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

### Using the Application

1. **Upload Documents**
   - Click "Upload" button in the sidebar
   - Select one or more PDF files
   - System processes asynchronously in background
   - Watch backend logs for processing progress

2. **Query Documents**
   - Type your question in the input box
   - System automatically routes to summary or detailed retrieval
   - Responses stream in real-time with markdown formatting

3. **Filter by Document** (Optional)
   - Click on a document in the sidebar to filter queries
   - Click "Clear Filter" to search across all documents

---

## üî¨ Technical Deep Dive

### Why Parent-Child Chunking?

Traditional RAG faces a dilemma:
- **Small chunks** (100-200 chars): Great for precise retrieval, but lack context for the LLM
- **Large chunks** (1000+ chars): Rich context, but poor retrieval accuracy

**Solution**: Store small chunks for search, retrieve large chunks for generation.

```python
# Child chunk (stored with embedding, used for search)
"The accuracy of the model was 94.2% on the test set."

# Parent chunk (retrieved for LLM context)
"We evaluated our proposed diabetes prediction model on a held-out 
test set of 5,000 patients. The accuracy of the model was 94.2% on 
the test set, with a precision of 91.3% and recall of 96.7%. This 
represents a 5.3% improvement over the baseline approach."
```

### Why HyDe (Hypothetical Document Embeddings)?

User queries and document text often have different linguistic patterns:

**Query**: "What is diabetes?"  
**Document**: "Diabetes mellitus is a chronic metabolic disorder..."

By transforming the query into a hypothetical answer first, we bridge this gap:

**Query** ‚Üí **HyDe Transform** ‚Üí "Diabetes is a chronic condition where the body cannot properly regulate blood sugar levels due to insulin resistance or deficiency..." ‚Üí **Embed & Search**

This transformation aligns the query's semantic representation with how the information appears in documents.

### Why Intelligent Routing?

Not all queries need the same retrieval strategy:

- **"Summarize this paper"** ‚Üí Best answered by the pre-generated summary
- **"What was the F1 score of model X?"** ‚Üí Needs precise chunk retrieval

The router compares L2 distances and makes this decision automatically:
```python
if summary_distance < (chunk_distance * 0.8):
    use_summary  # Broad question
else:
    use_parent_child  # Specific question
```

---

## üìä Performance Characteristics

### Latency Breakdown (Typical Query)
- **Query Transformation (HyDe)**: ~800ms (Gemini API call)
- **Vector Search**: ~50ms (PostgreSQL + pgvector)
- **Parent Retrieval**: ~20ms (PostgreSQL with deduplication)
- **Final Generation**: ~2-4s (Gemini streaming)
- **Total**: ~3-5s for complete response

### Scalability Considerations
- **Concurrent Users**: FastAPI async + Celery workers scale horizontally
- **Document Volume**: Tested with 10,000+ documents; pgvector handles millions of vectors
- **Embedding Generation**: Batched processing in Celery workers
- **Token Optimization**: Parent deduplication reduces LLM input by ~30%

---

## üîÆ Future Roadmap

### Phase 1: Enhanced Retrieval
- [ ] Hybrid search (BM25 + vector similarity)
- [ ] Re-ranking with cross-encoders
- [ ] Multi-query generation for better recall
- [ ] Configurable chunk sizes per document type

### Phase 2: Advanced Features
- [ ] Multi-modal support (images, tables from PDFs)
- [ ] Conversational memory and follow-up questions
- [ ] Citation tracking and source highlighting
- [ ] Query expansion techniques

### Phase 3: Production Hardening
- [ ] Authentication & authorization (OAuth2)
- [ ] Multi-tenancy support
- [ ] Rate limiting & quota management
- [ ] Monitoring with Prometheus + Grafana
- [ ] Kubernetes deployment manifests

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**Built for precision. Engineered for scale. Designed for real-world use.**

---

## ‚ú® Key Features

### üîç Advanced Retrieval Architecture

- **Parent-Child Hierarchical Chunking**
  - Small child chunks (500 chars) for precise vector search
  - Full parent paragraphs returned for comprehensive context
  - Eliminates precision-context trade-off
  - Unique parent deduplication to optimize token usage

- **HyDe Query Transformation**
  - Transforms user queries into hypothetical document passages
  - Bridges semantic gap between questions and answers
  - Uses Gemini API to generate contextually rich query representations
  - Significantly improves retrieval accuracy on ambiguous queries

- **Intelligent Query Routing**
  - Compares similarity scores between document summaries and child chunks
  - Routes broad/exploratory queries ‚Üí Document summaries
  - Routes specific/factual queries ‚Üí Parent-child retrieval
  - Dynamic threshold-based decision making (configurable)

### üèóÔ∏è Production-Grade Infrastructure

- **Asynchronous Document Processing**
  - Celery workers with Redis broker
  - Non-blocking uploads with background processing
  - Real-time task status tracking
  - Batch embedding generation for efficiency

- **Scalable Vector Storage**
  - PostgreSQL with pgvector extension
  - Optimized L2 distance queries
  - Separate tables for documents, parent chunks, child chunks, and summaries
  - Foreign key relationships for data integrity

- **Direct AI Integration**
  - Raw Gemini Python SDK for complete control
  - Custom prompt engineering for each pipeline stage
  - Streaming responses for better UX
  - No dependency on high-level RAG frameworks

### üé® Modern Full-Stack Application

- **React Frontend**: Modern UI with real-time streaming responses
- **FastAPI Backend**: High-performance async Python framework
- **Docker Compose**: One-command deployment
- **CORS Configured**: Secure cross-origin resource sharing

---

## üèõÔ∏è Technical Architecture

### Workflow 1: Asynchronous Data Ingestion Pipeline

This workflow handles document uploads, processing, and indexing without blocking the user.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User      ‚îÇ
‚îÇ  Uploads    ‚îÇ
‚îÇ   PDF(s)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        FastAPI Backend                          ‚îÇ
‚îÇ  - Accepts multiple files                                       ‚îÇ
‚îÇ  - Creates Document records                                     ‚îÇ
‚îÇ  - Dispatches Celery tasks                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Celery Worker + Redis                        ‚îÇ
‚îÇ  1. Extract text from PDF                                       ‚îÇ
‚îÇ  2. Clean text (remove formatting artifacts)                    ‚îÇ
‚îÇ  3. Generate document summary via Gemini API                    ‚îÇ
‚îÇ  4. Create embeddings for summary                               ‚îÇ
‚îÇ  5. Split into parent chunks (paragraphs)                       ‚îÇ
‚îÇ  6. Split parents into child chunks (500 chars, 100 overlap)    ‚îÇ
‚îÇ  7. Batch encode all child chunks (Sentence-Transformers)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              PostgreSQL + PGVector Database                     ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Tables:                                                        ‚îÇ
‚îÇ  ‚Ä¢ documents      ‚Üí Metadata (title, id)                        ‚îÇ
‚îÇ  ‚Ä¢ summaries      ‚Üí Summary text + embedding                    ‚îÇ
‚îÇ  ‚Ä¢ parents        ‚Üí Full paragraphs (document_id FK)            ‚îÇ
‚îÇ  ‚Ä¢ children       ‚Üí Small chunks + embeddings (parent_id FK)    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Indexes: Vector similarity (L2 distance) on embeddings         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Implementation Details**:
- Uses `pypdf` for text extraction with regex-based cleaning
- Gemini API generates comprehensive summaries (async)
- `sentence-transformers/all-MiniLM-L6-v2` creates 384-dim embeddings
- Parent chunks filtered by minimum length (50+ chars) to remove artifacts
- All embeddings stored as pgvector types for native similarity queries

---

### Workflow 2: Multi-Stage Retrieval & Generation Pipeline

This workflow handles user queries with a sophisticated 3-stage process.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User      ‚îÇ
‚îÇ   Query     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               Stage 1: Query Transformation (HyDe)              ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Original Query: "What is diabetes?"                            ‚îÇ
‚îÇ        ‚Üì                                                        ‚îÇ
‚îÇ  Gemini API: Generate hypothetical answer                      ‚îÇ
‚îÇ        ‚Üì                                                        ‚îÇ
‚îÇ  Transformed: "Diabetes is a chronic condition where the body  ‚îÇ
‚îÇ   cannot properly regulate blood sugar levels..."              ‚îÇ
‚îÇ        ‚Üì                                                        ‚îÇ
‚îÇ  Encode: queryVector = embed(transformed_text)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Stage 2: Intelligent Query Routing                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Parallel Search:                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ  ‚îÇ Summary Search     ‚îÇ    ‚îÇ Child Chunk Search  ‚îÇ             ‚îÇ
‚îÇ  ‚îÇ Top 1 summary      ‚îÇ    ‚îÇ Top 10 child chunks ‚îÇ             ‚îÇ
‚îÇ  ‚îÇ Distance: 0.82     ‚îÇ    ‚îÇ Distance: 1.25      ‚îÇ             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Routing Decision:                                              ‚îÇ
‚îÇ  if summary_distance < (chunk_distance * 0.8):                  ‚îÇ
‚îÇ      route = SUMMARY  # Broad question                          ‚îÇ
‚îÇ  else:                                                          ‚îÇ
‚îÇ      route = PARENT_CHILD  # Specific question                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Stage 3A: Summary-Based Response                   ‚îÇ
‚îÇ                      (Broad Questions)                          ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Context = Document Summary                                     ‚îÇ
‚îÇ  ‚Üì                                                              ‚îÇ
‚îÇ  Gemini API: Generate comprehensive answer from summary        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

       OR

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Stage 3B: Hierarchical Retrieval                     ‚îÇ
‚îÇ                   (Specific Questions)                          ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  1. Retrieve parent chunks for top 10 child chunks              ‚îÇ
‚îÇ  2. Deduplicate parent IDs (token optimization)                 ‚îÇ
‚îÇ  3. Combine child chunks + parent context                       ‚îÇ
‚îÇ  ‚Üì                                                              ‚îÇ
‚îÇ  Context = Child Chunks + Parent Paragraphs                     ‚îÇ
‚îÇ  ‚Üì                                                              ‚îÇ
‚îÇ  Gemini API: Generate detailed answer with citations           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Streaming Response                           ‚îÇ
‚îÇ  ‚Ä¢ Gemini API streams response chunks                           ‚îÇ
‚îÇ  ‚Ä¢ Frontend renders markdown progressively                      ‚îÇ
‚îÇ  ‚Ä¢ User sees real-time typing effect                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Implementation Details**:
- HyDe transformation uses custom prompt engineering with Gemini
- Query routing threshold (0.8) is configurable and tuned empirically
- Parent deduplication uses `set()` to eliminate redundant context
- Streaming uses Server-Sent Events (SSE) for real-time updates
- All prompts designed to prevent hallucination and enforce grounding

---

## üõ†Ô∏è Tech Stack

### Backend
- **FastAPI** - Modern async web framework with automatic OpenAPI docs
- **Celery** - Distributed task queue for async processing
- **Redis** - Message broker and result backend for Celery
- **PostgreSQL 16** - Primary relational database
- **pgvector** - Vector similarity search extension for PostgreSQL
- **SQLAlchemy** - ORM with typed mappings
- **Pydantic** - Data validation and settings management

### AI/ML
- **Google Gemini Python SDK** - Direct API integration for generation
- **Sentence-Transformers** - Local embedding generation (`all-MiniLM-L6-v2`)
- **pypdf** - PDF text extraction
- **NumPy** - Vector operations and distance calculations

### Frontend
- **React 18** - Modern UI library with hooks
- **Vite** - Fast build tool and dev server
- **CSS Modules** - Scoped styling
- **Fetch API** - Streaming response handling

### DevOps
- **Docker** - Containerization
- **Docker Compose** - Multi-container orchestration
- **Nginx** - Reverse proxy for production deployment
- **Git** - Version control

---

## üöÄ Setup & Installation

### Prerequisites
- Docker & Docker Compose (recommended)
- OR: Python 3.11+, Node.js 20+, PostgreSQL 16, Redis

### Environment Variables

Create a `.env` file in the root directory:

```bash
# Google Gemini API
GOOGLE_API_KEY=your_gemini_api_key_here

# Database
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_secure_password
POSTGRES_DB=personal_search_engine

# Celery
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0
```

### Quick Start with Docker (Recommended)

```bash
# Clone the repository
git clone https://github.com/Fusion831/Personal-Search-Engine.git
cd Personal-Search-Engine

# Create .env file with your credentials
cp .env.example .env
# Edit .env and add your GOOGLE_API_KEY

# Build and start all services
docker-compose up --build

# Access the application
# Frontend: http://localhost:5173
# Backend API: http://localhost:8000
# API Docs: http://localhost:8000/docs
```

The application will start:
- **Frontend** on port 5173
- **Backend API** on port 8000
- **PostgreSQL** on port 5432
- **Redis** on port 6379

### Manual Setup (Development)

#### Backend

```bash
cd backend

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Start PostgreSQL and Redis (via Docker or local install)
# Initialize database
python -c "from database import engine; from models import Base; Base.metadata.create_all(bind=engine)"

# Start FastAPI server
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# In a separate terminal, start Celery worker
celery -A worker.celery_app worker --loglevel=info
```

#### Frontend

```bash
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

### Using the Application

1. **Upload Documents**
   - Click "Upload" button
   - Select one or more PDF files
   - System processes asynchronously (watch backend logs for progress)

2. **Query Documents**
   - Type your question in the input box
   - System automatically routes to summary or detailed retrieval
   - Responses stream in real-time

3. **Filter by Document** (Optional)
   - Click on a document in the sidebar to filter queries to that document
   - Click "Clear Filter" to search across all documents

---

## üî¨ Technical Deep Dive

### Why Parent-Child Chunking?

Traditional RAG faces a dilemma:
- **Small chunks** (100-200 chars): Great for precise retrieval, but lack context for the LLM
- **Large chunks** (1000+ chars): Rich context, but poor retrieval accuracy

**Solution**: Store small chunks for search, retrieve large chunks for generation.

```python
# Child chunk (stored with embedding)
"The accuracy of the model was 94.2% on the test set."

# Parent chunk (retrieved for context)
"We evaluated our proposed diabetes prediction model on a held-out 
test set of 5,000 patients. The accuracy of the model was 94.2% on 
the test set, with a precision of 91.3% and recall of 96.7%. This 
represents a 5.3% improvement over the baseline approach."
```

### Why HyDe (Hypothetical Document Embeddings)?

User queries and document text often have different linguistic patterns:

**Query**: "What is diabetes?"  
**Document**: "Diabetes mellitus is a chronic metabolic disorder..."

By transforming the query into a hypothetical answer first, we bridge this gap:

**Query** ‚Üí **HyDe Transform** ‚Üí "Diabetes is a chronic condition where the body cannot properly regulate blood sugar levels due to insulin resistance or deficiency..." ‚Üí **Embed & Search**

### Why Intelligent Routing?

Not all queries need the same retrieval strategy:

- **"Summarize this paper"** ‚Üí Best answered by the pre-generated summary
- **"What was the F1 score of model X?"** ‚Üí Needs precise chunk retrieval

Our router uses L2 distance comparison to make this decision automatically.

---

## üìä Performance Characteristics

### Latency Breakdown (Typical Query)
- **Query Transformation (HyDe)**: ~800ms (Gemini API call)
- **Vector Search**: ~50ms (PostgreSQL + pgvector)
- **Parent Retrieval**: ~20ms (PostgreSQL)
- **Final Generation**: ~2-4s (Gemini streaming)
- **Total**: ~3-5s for complete response

### Scalability Considerations
- **Concurrent Users**: FastAPI async + Celery workers scale horizontally
- **Document Size**: 10,000+ documents tested; pgvector handles millions of vectors
- **Embedding Generation**: Batched processing in Celery workers
- **Token Optimization**: Parent deduplication reduces LLM input by ~30%

---

## üîÆ Future Roadmap

### Phase 1: Conversational RAG (In Progress)
- [ ] Multi-turn conversation memory
- [ ] Context window management for chat history
- [ ] Follow-up question understanding
- [ ] Conversation summarization for long threads

### Phase 2: Advanced Features
- [ ] Multi-modal support (images, tables from PDFs)
- [ ] Hybrid search (BM25 + vector similarity)
- [ ] Re-ranking with cross-encoders
- [ ] Query expansion techniques

### Phase 3: Production Hardening
- [ ] Authentication & authorization (OAuth2)
- [ ] Multi-tenancy support
- [ ] Rate limiting & quota management
- [ ] Monitoring with Prometheus + Grafana
- [ ] Kubernetes deployment manifests

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ü§ù Contributing

Contributions are welcome! This is a learning project, and I'm open to:
- Architecture improvements
- Performance optimizations
- Bug reports and fixes
- Documentation enhancements

Please open an issue first to discuss what you'd like to change.

---

## üë®‚Äçüíª Author

**Daksh Patel**  
Building production ML systems from first principles.

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/in/your-profile)
[![GitHub](https://img.shields.io/badge/GitHub-Follow-black)](https://github.com/Fusion831)

---

## üôè Acknowledgments

- Google Gemini team for the excellent Python SDK
- Sentence-Transformers for open-source embedding models
- The FastAPI and PostgreSQL communities
- All contributors to the open-source libraries used in this project

---

**Built with precision. Engineered for scale. Designed for impact.**
